{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, TimeDistributed, ConvLSTM2D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/RAW_A9_01/a9_clean.csv', header=None)\n",
    "# df = pd.read_csv('data/RAW_A9_02/a9_02_clean.csv', header=None)\n",
    "# df = pd.read_csv('data/RAW_A9_02/a9_02_above.csv', header=None)\n",
    "df = pd.read_csv('data/RAW_A9_03/a9_03_clean.csv', header=None)\n",
    "\n",
    "# df = pd.read_csv('data/train_data_UU.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total shape + label: (167, 502)\n"
     ]
    }
   ],
   "source": [
    "print(\"total shape + label:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Labels ]\n",
      "labelA shape: (76, 502)\n",
      "labelB shape: (17, 502)\n",
      "labelC shape: (74, 502)\n",
      "\n",
      "[ Labels in Train & Test sets ]\n",
      "trainA shape: (50, 502) , testA shape: (16, 502)\n",
      "trainB shape: (13, 502) , testB shape: (4, 502)\n",
      "trainC shape: (59, 502) , testC shape: (15, 502)\n",
      "\n",
      "[ Train & Test sets ]\n",
      "train set: 109 , test set: 31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 平均切分各個標籤至train test中\n",
    "\n",
    "TRAIN_SET_RATE = 0.8  # Rate to seperate train set and test set\n",
    "\n",
    "labelA =  df[df.iloc[:, -1] == 'nothing']\n",
    "labelB =  df[df.iloc[:, -1] == 'passing']\n",
    "labelC =  df[df.iloc[:, -1] == 'touching']\n",
    "assert len(labelA) + len(labelB) + len(labelC) == len(df)\n",
    "print(\"[ Labels ]\")\n",
    "print(\"labelA shape:\", labelA.shape)\n",
    "print(\"labelB shape:\", labelB.shape)\n",
    "print(\"labelC shape:\", labelC.shape)\n",
    "print()\n",
    "\n",
    "sfA = shuffle(labelA)\n",
    "trainA, testA = sfA[:int(len(sfA) * TRAIN_SET_RATE)], sfA[int(len(sfA) * TRAIN_SET_RATE):]\n",
    "trainA = trainA[:50]  # shrimp trainA size\n",
    "\n",
    "sfB = shuffle(labelB)\n",
    "trainB, testB = sfB[:int(len(sfB) * TRAIN_SET_RATE)], sfB[int(len(sfB) * TRAIN_SET_RATE):]\n",
    "\n",
    "sfC = shuffle(labelC)\n",
    "trainC, testC = sfC[:int(len(sfC) * TRAIN_SET_RATE)], sfC[int(len(sfC) * TRAIN_SET_RATE):]\n",
    "\n",
    "print(\"[ Labels in Train & Test sets ]\")\n",
    "print(\"trainA shape:\", trainA.shape, \", testA shape:\", testA.shape)\n",
    "print(\"trainB shape:\", trainB.shape, \", testB shape:\", testB.shape)\n",
    "print(\"trainC shape:\", trainC.shape, \", testC shape:\", testC.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# train_set = pd.concat([trainA, trainB, trainC])\n",
    "train_set = pd.concat([trainA, trainC])\n",
    "train_set = shuffle(train_set)\n",
    "# test_set = pd.concat([testA, testB, testC])\n",
    "test_set = pd.concat([testA, testC])\n",
    "test_set = shuffle(test_set)\n",
    "print(\"[ Train & Test sets ]\")\n",
    "print(\"train set:\", len(train_set), \", test set:\", len(test_set))\n",
    "print()\n",
    "    \n",
    "x_train, y_train = train_set.iloc[:, 0:-1], train_set.iloc[:, -1]\n",
    "x_test, y_test = test_set.iloc[:, 0:-1], test_set.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['nothing', 'passing', 'touching']\n",
    "\n",
    "y_test_index = y_test.index\n",
    "\n",
    "# LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(LABELS)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# Standardize\n",
    "# sc = StandardScaler()\n",
    "# sc.fit(x_train)\n",
    "# x_train = sc.transform(x_train)\n",
    "# x_test = sc.transform(x_test)\n",
    "\n",
    "# savgol\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "batch_size = 10\n",
    "verbose = 0  # verbose : 0, 1或2。日誌顯示模式。0 =安靜模式, 1 =進度條, 2 =每輪一行。\n",
    "time_step = 1\n",
    "feature_dim = x_train.shape[1]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "if isinstance(x_train, np.ndarray):\n",
    "    x_train_reshape = x_train.reshape((x_train.shape[0], time_step, x_train.shape[1]))\n",
    "    x_test_reshape = x_test.reshape((x_test.shape[0], time_step, x_test.shape[1]))\n",
    "else:\n",
    "    x_train_reshape = x_train.values.reshape((x_train.shape[0], time_step, x_train.shape[1]))\n",
    "    x_test_reshape = x_test.values.reshape((x_test.shape[0], time_step, x_test.shape[1]))\n",
    "print(\"[ Reshape ]\")\n",
    "print(\"train X:\", x_train_reshape.shape, \", train Y:\", y_train.shape)\n",
    "print(\"test X:\", x_test_reshape.shape, \", test Y:\", y_test.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(time_step, feature_dim)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(40))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))  # out_layer\n",
    "model.summary()\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['sparse_categorical_accuracy'])\n",
    "# model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\n",
    "    n_steps, n_length = 4, 32\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), \n",
    "                              input_shape=(None, n_length, n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "        \n",
    "\n",
    "def run_experiment(trainX, trainy, testX, testy, repeats=10):\n",
    "\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    \n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split=0.33\n",
    "\n",
    "history = model.fit(x_train_reshape,\n",
    "                    y_train,\n",
    "                    validation_split=validation_split, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=verbose)\n",
    "_, accuracy = model.evaluate(x_test_reshape, y_test)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS = ['nothing', 'passing', 'touching']\n",
    "LABELS = ['nothing', 'touching']\n",
    "le = LabelEncoder()\n",
    "le.fit(LABELS)\n",
    "\n",
    "y_pred = model.predict(x_test_reshape, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "# y_pred = le.inverse_transform(y_pred_bool)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool, zero_division=1, target_names=LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[:5])\n",
    "print(\"pred:\")\n",
    "print(y_pred_bool)\n",
    "print()\n",
    "print(\"true:\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
